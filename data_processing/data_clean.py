import pandas as pd
import os


def detect_issues_detailed(df: pd.DataFrame, schema_config: dict, report_file: str):
    """
    è¯¦ç»†çš„æ•°æ®æ£€æµ‹ï¼ŒåŒ…æ‹¬æ¯åˆ—çš„å®é™…å€¼èŒƒå›´
    """
    with open(report_file, "w", encoding="utf-8") as f:
        f.write("=== è¯¦ç»†æ•°æ®æ£€æµ‹æŠ¥å‘Š ===\n\n")
        f.write(f"åŸå§‹æ•°æ®è¡Œæ•°: {len(df)}\n")
        f.write(f"åŸå§‹æ•°æ®åˆ—æ•°: {len(df.columns)}\n\n")

        n_rows = len(df)
        any_issue = False

        for col, cfg in schema_config.items():
            if col not in df.columns:
                f.write(f"{col}: âŒ åˆ—ä¸å­˜åœ¨\n\n")
                continue

            series = df[col]
            null_count = series.isna().sum()
            null_ratio = null_count / n_rows

            # æ˜¾ç¤ºåˆ—çš„åŸºæœ¬ä¿¡æ¯
            f.write(f"{col}:\n")
            f.write(f"  éç©ºå€¼æ•°é‡: {n_rows - null_count}\n")
            f.write(f"  ç©ºå€¼æ¯”ä¾‹: {null_ratio:.2%}\n")

            if null_count < n_rows:  # å¦‚æœæœ‰éç©ºå€¼
                non_null_series = series.dropna()
                f.write(f"  å®é™…å€¼èŒƒå›´: [{non_null_series.min():.6f}, {non_null_series.max():.6f}]\n")
                if len(non_null_series.unique()) <= 10:  # å¯¹äºæšä¸¾å‹åˆ—
                    f.write(f"  å®é™…å”¯ä¸€å€¼: {sorted(non_null_series.unique())}\n")

            out_of_range_count = 0
            if "range" in cfg and null_count < n_rows:
                low, high = cfg["range"]
                out_of_range_count = (~series.between(low, high)).sum()
                if out_of_range_count > 0:
                    f.write(f"  âš ï¸ è¶…å‡ºèŒƒå›´[{low}, {high}]çš„æ•°é‡: {out_of_range_count}\n")
            elif "enum" in cfg and null_count < n_rows:
                allowed = set(cfg["enum"])
                actual_values = set(series.dropna().unique())
                unexpected_values = actual_values - allowed
                out_of_range_count = (~series.isin(allowed)).sum()
                if out_of_range_count > 0:
                    f.write(f"  âš ï¸ æœŸæœ›æšä¸¾å€¼: {allowed}\n")
                    f.write(f"  âš ï¸ å®é™…æšä¸¾å€¼: {actual_values}\n")
                    f.write(f"  âš ï¸ æ„å¤–å€¼: {unexpected_values}\n")
                    f.write(f"  âš ï¸ éæ³•å€¼æ•°é‡: {out_of_range_count}\n")

            total_issues = null_count + out_of_range_count
            if total_issues > 0:
                any_issue = True
                f.write(f"  âš ï¸ æ€»é—®é¢˜æ•°é‡: {total_issues}\n")

            f.write("\n")

        if not any_issue:
            f.write("âœ… å…¨éƒ¨ç¬¦åˆè§„èŒƒ\n")


def clean_dataframe_gentle(df: pd.DataFrame, schema_config: dict) -> pd.DataFrame:
    """
    æ¸©å’Œçš„æ•°æ®æ¸…æ´—ï¼šç”±äºæ•°æ®å·²ç»å½’ä¸€åŒ–ï¼Œä¸»è¦å¤„ç†ç¼ºå¤±å€¼å’Œæšä¸¾å€¼
    """
    df_clean = df.copy()

    # å¤„ç†æšä¸¾å€¼
    for col, cfg in schema_config.items():
        if col not in df_clean.columns:
            continue

        if "enum" in cfg:
            # å¯¹äºæšä¸¾åˆ—ï¼Œå°†ä¸åœ¨æšä¸¾ä¸­çš„å€¼è®¾ä¸ºé»˜è®¤å€¼æˆ–ç¬¬ä¸€ä¸ªæšä¸¾å€¼
            enum_values = cfg["enum"]
            default_value = cfg.get("default", enum_values[0])
            df_clean[col] = df_clean[col].apply(lambda x: x if x in enum_values else default_value)

    # å¤„ç†ç¼ºå¤±å€¼
    for col, cfg in schema_config.items():
        if col not in df_clean.columns:
            continue

        if df_clean[col].isna().any():
            if "default" in cfg:
                df_clean[col] = df_clean[col].fillna(cfg["default"])
            elif "enum" in cfg:
                # å¯¹äºæšä¸¾åˆ—ï¼Œç”¨æœ€å¸¸è§çš„å€¼å¡«å……
                most_common = df_clean[col].mode()
                if len(most_common) > 0:
                    df_clean[col] = df_clean[col].fillna(most_common[0])
            else:
                # å¯¹äºæ•°å€¼åˆ—ï¼Œç”¨ä¸­ä½æ•°å¡«å……ï¼ˆæ³¨æ„ï¼šå·²ç»æ˜¯å½’ä¸€åŒ–åçš„æ•°æ®ï¼‰
                if pd.api.types.is_numeric_dtype(df_clean[col]):
                    df_clean[col] = df_clean[col].fillna(df_clean[col].median())

    # åªä¿ç•™ schema_config ä¸­å­˜åœ¨çš„åˆ—
    available_cols = [col for col in schema_config.keys() if col in df_clean.columns]

    return df_clean[available_cols]


def process_single_file(input_file: str, output_dir: str, schema_config: dict):
    """
    å¤„ç†å•ä¸ªCSVæ–‡ä»¶ - ç§»é™¤äº†å½’ä¸€åŒ–æ­¥éª¤
    """
    print(f"ğŸ” æ­£åœ¨å¤„ç†æ–‡ä»¶: {os.path.basename(input_file)}")

    try:
        # è¯»å–æ–‡ä»¶
        df = pd.read_csv(input_file)

        print(f"   åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")

        # æ£€æŸ¥å¿…è¦çš„åˆ—
        required_cols = ['number', 'merged']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            print(f"   âš ï¸ ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
            return False

        # ç”Ÿæˆè¯¦ç»†æ£€æµ‹æŠ¥å‘Š
        base_name = os.path.splitext(os.path.basename(input_file))[0]
        report_file = os.path.join(output_dir, f"{base_name}_clean_report.txt")
        detect_issues_detailed(df, schema_config, report_file)

        # æ‰§è¡Œæ¸©å’Œæ¸…æ´—ï¼ˆä¸»è¦å¤„ç†ç¼ºå¤±å€¼å’Œæšä¸¾å€¼ï¼‰
        cleaned = clean_dataframe_gentle(df, schema_config)

        print(f"   æ¸…æ´—åæ•°æ®å½¢çŠ¶: {cleaned.shape}")

        if len(cleaned) == 0:
            print("   âŒ æ¸…æ´—åæ•°æ®ä¸ºç©º")
            return False

        # æ³¨æ„ï¼šä¸å†è¿›è¡Œå½’ä¸€åŒ–ï¼Œå› ä¸ºæ•°æ®å·²ç»åœ¨åˆè¡¨æ—¶å½’ä¸€åŒ–è¿‡äº†

        # è¾“å‡ºæœ€ç»ˆç»“æœ
        output_file = os.path.join(output_dir, f"cleaned_{base_name}.csv")
        cleaned.to_csv(output_file, index=False)

        print(f"âœ… å®Œæˆæ¸…æ´—: {os.path.basename(output_file)}")
        print(f"   æœ€ç»ˆæ•°æ®å½¢çŠ¶: {cleaned.shape}")
        return True

    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶ {input_file} æ—¶å‡ºé”™: {e}")
        import traceback
        print(f"   è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False


def main():
    # è®¾ç½®è·¯å¾„
    input_folder = r"E:\codes\MachineLearning\Lab2\all_features\renamed_features\merged_datasets"
    output_folder = os.path.join(input_folder, "cleaned_data")

    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    # è°ƒæ•´èŒƒå›´è®¾ç½®ä»¥é€‚åº”å½’ä¸€åŒ–åçš„æ•°æ® [0, 1] èŒƒå›´
    schema_config = {
        # æ ‡è¯†åˆ—
        "number": {"type": "int", "required": True},

        # ä»£ç å˜æ›´ç‰¹å¾ - è°ƒæ•´ä¸ºå½’ä¸€åŒ–åçš„èŒƒå›´
        "directory_num": {"range": (0, 1.1)},  # ç¨å¾®è¶…è¿‡1ä»¥å®¹é”™
        "language_num": {"range": (0, 1.1)},
        "file_type": {"range": (0, 1.1)},
        "has_test": {"enum": [0, 1], "default": 0},

        # æ–‡æœ¬å†…å®¹ç‰¹å¾
        "has_feature": {"enum": [0, 1], "default": 0},
        "has_bug": {"enum": [0, 1], "default": 0},
        "has_document": {"enum": [0, 1], "default": 0},
        "has_improve": {"enum": [0, 1], "default": 0},
        "has_refactor": {"enum": [0, 1], "default": 0},
        "subject_length": {"range": (0, 1.1)},
        "subject_readability": {"range": (0, 1.1)},
        "message_length": {"range": (0, 1.1)},
        "message_readability": {"range": (0, 1.1)},

        # ä»£ç å˜æ›´è§„æ¨¡ - è°ƒæ•´ä¸ºå½’ä¸€åŒ–åçš„èŒƒå›´
        "lines_added": {"range": (0, 1.1)},
        "lines_deleted": {"range": (0, 1.1)},
        "segs_added": {"range": (0, 1.1)},
        "segs_deleted": {"range": (0, 1.1)},
        "segs_updated": {"range": (0, 1.1)},
        "files_added": {"range": (0, 1.1)},
        "files_deleted": {"range": (0, 1.1)},
        "files_updated": {"range": (0, 1.1)},
        "modify_proportion": {"range": (0, 1.1)},
        "modify_entropy": {"range": (0, 1.1)},
        "test_churn": {"range": (0, 1.1)},
        "non_test_churn": {"range": (0, 1.1)},

        # è¯„å®¡ç›¸å…³ç‰¹å¾
        "reviewer_num": {"range": (0, 1.1)},
        "bot_reviewer_num": {"range": (0, 1.1)},
        "has_reviewed": {"enum": [0, 1], "default": 0},
        "comment_num": {"range": (0, 1.1)},
        "comment_length": {"range": (0, 1.1)},
        "last_comment_mention": {"enum": [0, 1], "default": 0},

        # ç›®æ ‡å˜é‡
        "merged": {"enum": [0, 1], "default": 0},
    }

    # è·å–æ‰€æœ‰åˆå¹¶åçš„CSVæ–‡ä»¶
    csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]

    print(f"ğŸ” æ‰¾åˆ° {len(csv_files)} ä¸ªæ•°æ®é›†æ–‡ä»¶")
    print("å¼€å§‹æ‰¹é‡æ¸…æ´—æ•°æ®...\n")

    success_count = 0
    failed_files = []

    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for file in csv_files:
        file_path = os.path.join(input_folder, file)
        if process_single_file(file_path, output_folder, schema_config):
            success_count += 1
        else:
            failed_files.append(file)
        print()  # ç©ºè¡Œåˆ†éš”

    # è¾“å‡ºæ€»ç»“
    print(f"{'=' * 50}")
    print("ğŸ“Š æ‰¹é‡å¤„ç†æ€»ç»“:")
    print(f"âœ… æˆåŠŸå¤„ç†: {success_count} ä¸ªæ–‡ä»¶")
    print(f"âŒ å¤„ç†å¤±è´¥: {len(failed_files)} ä¸ªæ–‡ä»¶")
    if failed_files:
        print("å¤±è´¥æ–‡ä»¶åˆ—è¡¨:")
        for file in failed_files:
            print(f"   - {file}")
    print(f"ğŸ“ æ¸…æ´—åçš„æ–‡ä»¶ä¿å­˜åœ¨: {output_folder}")


if __name__ == "__main__":
    main()